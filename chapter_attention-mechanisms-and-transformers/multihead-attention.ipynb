{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3cdf46fe",
      "metadata": {
        "id": "3cdf46fe"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b2f1e735",
      "metadata": {
        "id": "b2f1e735",
        "outputId": "f3c182b6-600c-4e37-9e61-b524a2f5d196",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting d2l==1.0.0-alpha1.post0\n",
            "  Downloading d2l-1.0.0a1.post0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.3.5)\n",
            "Collecting jupyter\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (2.23.0)\n",
            "Collecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (3.2.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (3.9.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.5.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (6.1.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.3.4)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (7.7.1)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.3.2-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 29.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (5.1.1)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (7.9.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 66.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.0.10)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.0.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.11.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (23.2.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.13.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (1.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (1.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (5.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.6.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.16.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (22.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==1.0.0-alpha1.post0) (2022.4)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.2.1-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 830 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->d2l==1.0.0-alpha1.post0) (21.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2022.9.24)\n",
            "Installing collected packages: jedi, qtpy, qtconsole, matplotlib-inline, jupyter, d2l\n",
            "Successfully installed d2l-1.0.0a1.post0 jedi-0.18.1 jupyter-1.0.0 matplotlib-inline-0.1.6 qtconsole-5.3.2 qtpy-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d2f7e9",
      "metadata": {
        "origin_pos": 1,
        "id": "56d2f7e9"
      },
      "source": [
        "# Multi-Head Attention\n",
        ":label:`sec_multihead-attention`\n",
        "\n",
        "\n",
        "In practice,\n",
        "given the same set of queries, keys, and values\n",
        "we may want our model to\n",
        "combine knowledge from\n",
        "different behaviors of the same attention mechanism,\n",
        "such as capturing dependencies of various ranges (e.g., shorter-range vs. longer-range)\n",
        "within a sequence.\n",
        "Thus, \n",
        "it may be beneficial \n",
        "to allow our attention mechanism\n",
        "to jointly use different representation subspaces\n",
        "of queries, keys, and values.\n",
        "\n",
        "\n",
        "\n",
        "To this end,\n",
        "instead of performing a single attention pooling,\n",
        "queries, keys, and values\n",
        "can be transformed\n",
        "with $h$ independently learned linear projections.\n",
        "Then these $h$ projected queries, keys, and values\n",
        "are fed into attention pooling in parallel.\n",
        "In the end,\n",
        "$h$ attention pooling outputs\n",
        "are concatenated and \n",
        "transformed with another learned linear projection\n",
        "to produce the final output.\n",
        "This design\n",
        "is called *multi-head attention*,\n",
        "where each of the $h$ attention pooling outputs\n",
        "is a *head* :cite:`Vaswani.Shazeer.Parmar.ea.2017`.\n",
        "Using fully connected layers\n",
        "to perform learnable linear transformations,\n",
        ":numref:`fig_multi-head-attention`\n",
        "describes multi-head attention.\n",
        "\n",
        "![Multi-head attention, where multiple heads are concatenated then linearly transformed.](https://github.com/d2l-ai/d2l-pytorch-colab/blob/master/img/multi-head-attention.svg?raw=1)\n",
        ":label:`fig_multi-head-attention`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Model\n",
        "\n",
        "Before providing the implementation of multi-head attention,\n",
        "let's formalize this model mathematically.\n",
        "Given a query $\\mathbf{q} \\in \\mathbb{R}^{d_q}$,\n",
        "a key $\\mathbf{k} \\in \\mathbb{R}^{d_k}$,\n",
        "and a value $\\mathbf{v} \\in \\mathbb{R}^{d_v}$,\n",
        "each attention head $\\mathbf{h}_i$  ($i = 1, \\ldots, h$)\n",
        "is computed as\n",
        "\n",
        "$$\\mathbf{h}_i = f(\\mathbf W_i^{(q)}\\mathbf q, \\mathbf W_i^{(k)}\\mathbf k,\\mathbf W_i^{(v)}\\mathbf v) \\in \\mathbb R^{p_v},$$\n",
        "\n",
        "where learnable parameters\n",
        "$\\mathbf W_i^{(q)}\\in\\mathbb R^{p_q\\times d_q}$,\n",
        "$\\mathbf W_i^{(k)}\\in\\mathbb R^{p_k\\times d_k}$\n",
        "and $\\mathbf W_i^{(v)}\\in\\mathbb R^{p_v\\times d_v}$,\n",
        "and\n",
        "$f$ is attention pooling,\n",
        "such as\n",
        "additive attention and scaled dot-product attention\n",
        "in :numref:`sec_attention-scoring-functions`.\n",
        "The multi-head attention output\n",
        "is another linear transformation via \n",
        "learnable parameters\n",
        "$\\mathbf W_o\\in\\mathbb R^{p_o\\times h p_v}$\n",
        "of the concatenation of $h$ heads:\n",
        "\n",
        "$$\\mathbf W_o \\begin{bmatrix}\\mathbf h_1\\\\\\vdots\\\\\\mathbf h_h\\end{bmatrix} \\in \\mathbb{R}^{p_o}.$$\n",
        "\n",
        "Based on this design,\n",
        "each head may attend to different parts of the input.\n",
        "More sophisticated functions than the simple weighted average\n",
        "can be expressed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5d5214a7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:09:45.192417Z",
          "iopub.status.busy": "2022-09-07T22:09:45.191505Z",
          "iopub.status.idle": "2022-09-07T22:09:47.493915Z",
          "shell.execute_reply": "2022-09-07T22:09:47.493046Z"
        },
        "origin_pos": 3,
        "tab": [
          "pytorch"
        ],
        "id": "5d5214a7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f0fa69",
      "metadata": {
        "origin_pos": 5,
        "id": "55f0fa69"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "In our implementation,\n",
        "we [**choose the scaled dot-product attention\n",
        "for each head**] of the multi-head attention.\n",
        "To avoid significant growth\n",
        "of computational cost and parameterization cost,\n",
        "we set\n",
        "$p_q = p_k = p_v = p_o / h$.\n",
        "Note that $h$ heads\n",
        "can be computed in parallel\n",
        "if we set\n",
        "the number of outputs of linear transformations\n",
        "for the query, key, and value\n",
        "to $p_q h = p_k h = p_v h = p_o$.\n",
        "In the following implementation,\n",
        "$p_o$ is specified via the argument `num_hiddens`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5b8d94aa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:09:47.500758Z",
          "iopub.status.busy": "2022-09-07T22:09:47.500123Z",
          "iopub.status.idle": "2022-09-07T22:09:47.509476Z",
          "shell.execute_reply": "2022-09-07T22:09:47.508753Z"
        },
        "origin_pos": 7,
        "tab": [
          "pytorch"
        ],
        "id": "5b8d94aa"
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "class MultiHeadAttention(d2l.Module):\n",
        "    \"\"\"Multi-head attention.\"\"\"\n",
        "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = d2l.DotProductAttention(dropout, num_heads)\n",
        "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
        "\n",
        "    def forward(self, queries, keys, values, valid_lens, window_mask=None):\n",
        "        # Shape of queries, keys, or values:\n",
        "        # (batch_size, no. of queries or key-value pairs, num_hiddens)\n",
        "        # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
        "        # After transposing, shape of output queries, keys, or values:\n",
        "        # (batch_size * num_heads, no. of queries or key-value pairs,\n",
        "        # num_hiddens / num_heads)\n",
        "        queries = self.transpose_qkv(self.W_q(queries))\n",
        "        keys = self.transpose_qkv(self.W_k(keys))\n",
        "        values = self.transpose_qkv(self.W_v(values))\n",
        "\n",
        "        if valid_lens is not None:\n",
        "            # On axis 0, copy the first item (scalar or vector) for num_heads\n",
        "            # times, then copy the next item, and so on\n",
        "            valid_lens = torch.repeat_interleave(\n",
        "                valid_lens, repeats=self.num_heads, dim=0)\n",
        "\n",
        "        # Shape of output: (batch_size * num_heads, no. of queries,\n",
        "        # num_hiddens / num_heads)\n",
        "        output = self.attention(queries, keys, values, valid_lens,\n",
        "                                window_mask)\n",
        "        # Shape of output_concat: (batch_size, no. of queries, num_hiddens)\n",
        "        output_concat = self.transpose_output(output)\n",
        "        return self.W_o(output_concat)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f500682",
      "metadata": {
        "origin_pos": 9,
        "id": "9f500682"
      },
      "source": [
        "To allow for [**parallel computation of multiple heads**],\n",
        "the above `MultiHeadAttention` class uses two transposition methods as defined below.\n",
        "Specifically,\n",
        "the `transpose_output` method reverses the operation\n",
        "of the `transpose_qkv` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fcf58b93",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:09:47.512886Z",
          "iopub.status.busy": "2022-09-07T22:09:47.512384Z",
          "iopub.status.idle": "2022-09-07T22:09:47.518610Z",
          "shell.execute_reply": "2022-09-07T22:09:47.517855Z"
        },
        "origin_pos": 11,
        "tab": [
          "pytorch"
        ],
        "id": "fcf58b93"
      },
      "outputs": [],
      "source": [
        "@d2l.add_to_class(MultiHeadAttention)  #@save\n",
        "def transpose_qkv(self, X):\n",
        "    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n",
        "    # Shape of input X: (batch_size, no. of queries or key-value pairs,\n",
        "    # num_hiddens). Shape of output X: (batch_size, no. of queries or\n",
        "    # key-value pairs, num_heads, num_hiddens / num_heads)\n",
        "    X = X.reshape(X.shape[0], X.shape[1], self.num_heads, -1)\n",
        "    # Shape of output X: (batch_size, num_heads, no. of queries or key-value\n",
        "    # pairs, num_hiddens / num_heads)\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    # Shape of output: (batch_size * num_heads, no. of queries or key-value\n",
        "    # pairs, num_hiddens / num_heads)\n",
        "    return X.reshape(-1, X.shape[2], X.shape[3])\n",
        "\n",
        "@d2l.add_to_class(MultiHeadAttention)  #@save\n",
        "def transpose_output(self, X):\n",
        "    \"\"\"Reverse the operation of transpose_qkv.\"\"\"\n",
        "    X = X.reshape(-1, self.num_heads, X.shape[1], X.shape[2])\n",
        "    X = X.permute(0, 2, 1, 3)\n",
        "    return X.reshape(X.shape[0], X.shape[1], -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7830649",
      "metadata": {
        "origin_pos": 13,
        "id": "a7830649"
      },
      "source": [
        "Let's [**test our implemented**] `MultiHeadAttention` class\n",
        "using a toy example where keys and values are the same.\n",
        "As a result,\n",
        "the shape of the multi-head attention output\n",
        "is (`batch_size`, `num_queries`, `num_hiddens`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e36b7e42",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:09:47.521921Z",
          "iopub.status.busy": "2022-09-07T22:09:47.521424Z",
          "iopub.status.idle": "2022-09-07T22:09:47.527032Z",
          "shell.execute_reply": "2022-09-07T22:09:47.526227Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "e36b7e42",
        "outputId": "978e9046-8505-4d83-a963-daccb0c850cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "num_hiddens, num_heads = 100, 5\n",
        "attention = MultiHeadAttention(num_hiddens, num_heads, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "231f2609",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:09:47.529885Z",
          "iopub.status.busy": "2022-09-07T22:09:47.529618Z",
          "iopub.status.idle": "2022-09-07T22:09:47.560888Z",
          "shell.execute_reply": "2022-09-07T22:09:47.560057Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "231f2609"
      },
      "outputs": [],
      "source": [
        "batch_size, num_queries, num_kvpairs, valid_lens = 2, 4, 6, torch.tensor([3, 2])\n",
        "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
        "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
        "d2l.check_shape(attention(X, Y, Y, valid_lens),\n",
        "                (batch_size, num_queries, num_hiddens))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecd6c5ae",
      "metadata": {
        "origin_pos": 19,
        "id": "ecd6c5ae"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Multi-head attention combines knowledge of the same attention pooling via different representation subspaces of queries, keys, and values.\n",
        "* To compute multiple heads of multi-head attention in parallel, proper tensor manipulation is needed.\n",
        "\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Visualize attention weights of multiple heads in this experiment.\n",
        "1. Suppose that we have a trained model based on multi-head attention and we want to prune least important attention heads to increase the prediction speed. How can we design experiments to measure the importance of an attention head?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e075c036",
      "metadata": {
        "origin_pos": 21,
        "tab": [
          "pytorch"
        ],
        "id": "e075c036"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1635)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}