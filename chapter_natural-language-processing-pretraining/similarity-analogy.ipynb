{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6653f2c1",
      "metadata": {
        "id": "6653f2c1"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "519a037e",
      "metadata": {
        "id": "519a037e",
        "outputId": "00e0f229-10e3-4831-e68a-8ebb2542ac7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting d2l==1.0.0-alpha1.post0\n",
            "  Downloading d2l-1.0.0a1.post0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 892 kB/s \n",
            "\u001b[?25hCollecting jupyter\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (3.2.2)\n",
            "Collecting matplotlib-inline\n",
            "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (0.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==1.0.0-alpha1.post0) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym->d2l==1.0.0-alpha1.post0) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym->d2l==1.0.0-alpha1.post0) (4.1.1)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.3.2-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.3.4)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (5.5.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==1.0.0-alpha1.post0) (7.7.1)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (7.9.0)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (6.1.12)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (57.4.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.0.10)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 33.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (1.15.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.0.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (3.6.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==1.0.0-alpha1.post0) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.11.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (23.2.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.13.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.11.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==1.0.0-alpha1.post0) (1.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter->d2l==1.0.0-alpha1.post0) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==1.0.0-alpha1.post0) (3.0.9)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (5.0.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (2.16.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (22.1.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter->d2l==1.0.0-alpha1.post0) (0.18.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==1.0.0-alpha1.post0) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==1.0.0-alpha1.post0) (2022.4)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.2.1-py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 793 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter->d2l==1.0.0-alpha1.post0) (21.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==1.0.0-alpha1.post0) (2022.9.24)\n",
            "Installing collected packages: jedi, qtpy, qtconsole, matplotlib-inline, jupyter, d2l\n",
            "Successfully installed d2l-1.0.0a1.post0 jedi-0.18.1 jupyter-1.0.0 matplotlib-inline-0.1.6 qtconsole-5.3.2 qtpy-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l==1.0.0-alpha1.post0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "315f602f",
      "metadata": {
        "origin_pos": 0,
        "id": "315f602f"
      },
      "source": [
        "# Word Similarity and Analogy\n",
        ":label:`sec_synonyms`\n",
        "\n",
        "In :numref:`sec_word2vec_pretraining`, \n",
        "we trained a word2vec model on a small dataset, \n",
        "and applied it\n",
        "to find semantically similar words \n",
        "for an input word.\n",
        "In practice,\n",
        "word vectors that are pretrained\n",
        "on large corpora can be\n",
        "applied to downstream\n",
        "natural language processing tasks,\n",
        "which will be covered later\n",
        "in :numref:`chap_nlp_app`.\n",
        "To demonstrate \n",
        "semantics of pretrained word vectors\n",
        "from large corpora in a straightforward way,\n",
        "let's apply them\n",
        "in the word similarity and analogy tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d3c7d7ad",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:03:48.958712Z",
          "iopub.status.busy": "2022-09-07T22:03:48.958109Z",
          "iopub.status.idle": "2022-09-07T22:03:50.838554Z",
          "shell.execute_reply": "2022-09-07T22:03:50.837720Z"
        },
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "d3c7d7ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee5bd861",
      "metadata": {
        "origin_pos": 3,
        "id": "ee5bd861"
      },
      "source": [
        "## Loading Pretrained Word Vectors\n",
        "\n",
        "Below lists pretrained GloVe embeddings of dimension 50, 100, and 300,\n",
        "which can be downloaded from the [GloVe website](https://nlp.stanford.edu/projects/glove/).\n",
        "The pretrained fastText embeddings are available in multiple languages.\n",
        "Here we consider one English version (300-dimensional \"wiki.en\") that can be downloaded from the\n",
        "[fastText website](https://fasttext.cc/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7a2bd106",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:03:50.843876Z",
          "iopub.status.busy": "2022-09-07T22:03:50.843334Z",
          "iopub.status.idle": "2022-09-07T22:03:50.848399Z",
          "shell.execute_reply": "2022-09-07T22:03:50.847667Z"
        },
        "origin_pos": 4,
        "tab": [
          "pytorch"
        ],
        "id": "7a2bd106"
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n",
        "                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n",
        "                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n",
        "                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n",
        "                           'c1816da3821ae9f43899be655002f6c723e91b88')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e8191c",
      "metadata": {
        "origin_pos": 5,
        "id": "c9e8191c"
      },
      "source": [
        "To load these pretrained GloVe and fastText embeddings, we define the following `TokenEmbedding` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fb20e6e8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:03:50.852790Z",
          "iopub.status.busy": "2022-09-07T22:03:50.852363Z",
          "iopub.status.idle": "2022-09-07T22:03:50.860752Z",
          "shell.execute_reply": "2022-09-07T22:03:50.860035Z"
        },
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "fb20e6e8"
      },
      "outputs": [],
      "source": [
        "#@save\n",
        "class TokenEmbedding:\n",
        "    \"\"\"Token Embedding.\"\"\"\n",
        "    def __init__(self, embedding_name):\n",
        "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
        "            embedding_name)\n",
        "        self.unknown_idx = 0\n",
        "        self.token_to_idx = {token: idx for idx, token in\n",
        "                             enumerate(self.idx_to_token)}\n",
        "\n",
        "    def _load_embedding(self, embedding_name):\n",
        "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
        "        data_dir = d2l.download_extract(embedding_name)\n",
        "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
        "        # fastText website: https://fasttext.cc/\n",
        "        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                elems = line.rstrip().split(' ')\n",
        "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
        "                # Skip header information, such as the top row in fastText\n",
        "                if len(elems) > 1:\n",
        "                    idx_to_token.append(token)\n",
        "                    idx_to_vec.append(elems)\n",
        "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
        "        return idx_to_token, torch.tensor(idx_to_vec)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
        "                   for token in tokens]\n",
        "        vecs = self.idx_to_vec[torch.tensor(indices)]\n",
        "        return vecs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e349b2e0",
      "metadata": {
        "origin_pos": 7,
        "id": "e349b2e0"
      },
      "source": [
        "Below we load the\n",
        "50-dimensional GloVe embeddings\n",
        "(pretrained on a Wikipedia subset).\n",
        "When creating the `TokenEmbedding` instance,\n",
        "the specified embedding file has to be downloaded if it\n",
        "was not yet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f13812fe",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:03:50.864928Z",
          "iopub.status.busy": "2022-09-07T22:03:50.864515Z",
          "iopub.status.idle": "2022-09-07T22:04:01.591253Z",
          "shell.execute_reply": "2022-09-07T22:04:01.590090Z"
        },
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "f13812fe",
        "outputId": "004ecc05-0600-4722-906b-4127a3c72089",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading ../data/glove.6B.50d.zip from http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip...\n"
          ]
        }
      ],
      "source": [
        "glove_6b50d = TokenEmbedding('glove.6b.50d')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "362fbf44",
      "metadata": {
        "origin_pos": 9,
        "id": "362fbf44"
      },
      "source": [
        "Output the vocabulary size. The vocabulary contains 400000 words (tokens) and a special unknown token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7cc78c44",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.597287Z",
          "iopub.status.busy": "2022-09-07T22:04:01.596549Z",
          "iopub.status.idle": "2022-09-07T22:04:01.606632Z",
          "shell.execute_reply": "2022-09-07T22:04:01.605645Z"
        },
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "7cc78c44",
        "outputId": "c8ce3eb7-a2eb-4b9d-917a-fdbefdb1bf7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400001"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "321b1bad",
      "metadata": {
        "origin_pos": 11,
        "id": "321b1bad"
      },
      "source": [
        "We can get the index of a word in the vocabulary, and vice versa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4ef75826",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.613125Z",
          "iopub.status.busy": "2022-09-07T22:04:01.612359Z",
          "iopub.status.idle": "2022-09-07T22:04:01.619737Z",
          "shell.execute_reply": "2022-09-07T22:04:01.618765Z"
        },
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "4ef75826",
        "outputId": "452df328-5454-4852-b605-b710e0129ec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3367, 'beautiful')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1152650e",
      "metadata": {
        "origin_pos": 13,
        "id": "1152650e"
      },
      "source": [
        "## Applying Pretrained Word Vectors\n",
        "\n",
        "Using the loaded GloVe vectors,\n",
        "we will demonstrate their semantics\n",
        "by applying them\n",
        "in the following word similarity and analogy tasks.\n",
        "\n",
        "\n",
        "### Word Similarity\n",
        "\n",
        "Similar to :numref:`subsec_apply-word-embed`,\n",
        "in order to find semantically similar words\n",
        "for an input word\n",
        "based on cosine similarities between\n",
        "word vectors,\n",
        "we implement the following `knn`\n",
        "($k$-nearest neighbors) function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5551a598",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.624984Z",
          "iopub.status.busy": "2022-09-07T22:04:01.624158Z",
          "iopub.status.idle": "2022-09-07T22:04:01.632549Z",
          "shell.execute_reply": "2022-09-07T22:04:01.631242Z"
        },
        "origin_pos": 15,
        "tab": [
          "pytorch"
        ],
        "id": "5551a598"
      },
      "outputs": [],
      "source": [
        "def knn(W, x, k):\n",
        "    # Add 1e-9 for numerical stability\n",
        "    cos = torch.mv(W, x.reshape(-1,)) / (\n",
        "        torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) *\n",
        "        torch.sqrt((x * x).sum()))\n",
        "    _, topk = torch.topk(cos, k=k)\n",
        "    return topk, [cos[int(i)] for i in topk]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6285a4da",
      "metadata": {
        "origin_pos": 16,
        "id": "6285a4da"
      },
      "source": [
        "Then, we \n",
        "search for similar words\n",
        "using the pretrained word vectors \n",
        "from the `TokenEmbedding` instance `embed`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "1bafb264",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.636816Z",
          "iopub.status.busy": "2022-09-07T22:04:01.635936Z",
          "iopub.status.idle": "2022-09-07T22:04:01.642402Z",
          "shell.execute_reply": "2022-09-07T22:04:01.641444Z"
        },
        "origin_pos": 17,
        "tab": [
          "pytorch"
        ],
        "id": "1bafb264"
      },
      "outputs": [],
      "source": [
        "def get_similar_tokens(query_token, k, embed):\n",
        "    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n",
        "    for i, c in zip(topk[1:], cos[1:]):  # Exclude the input word\n",
        "        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e8e4552",
      "metadata": {
        "origin_pos": 18,
        "id": "1e8e4552"
      },
      "source": [
        "The vocabulary of the pretrained word vectors\n",
        "in `glove_6b50d` contains 400000 words and a special unknown token. \n",
        "Excluding the input word and unknown token,\n",
        "among this vocabulary\n",
        "let's find \n",
        "three most semantically similar words\n",
        "to word \"chip\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d71d66a7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.647552Z",
          "iopub.status.busy": "2022-09-07T22:04:01.646946Z",
          "iopub.status.idle": "2022-09-07T22:04:01.695905Z",
          "shell.execute_reply": "2022-09-07T22:04:01.694917Z"
        },
        "origin_pos": 19,
        "tab": [
          "pytorch"
        ],
        "id": "d71d66a7",
        "outputId": "4631ffe3-536e-4f99-d2d7-f0c9617c5e14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine sim=0.856: chips\n",
            "cosine sim=0.749: intel\n",
            "cosine sim=0.749: electronics\n"
          ]
        }
      ],
      "source": [
        "get_similar_tokens('chip', 3, glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f267046",
      "metadata": {
        "origin_pos": 20,
        "id": "5f267046"
      },
      "source": [
        "Below outputs similar words\n",
        "to \"baby\" and \"beautiful\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b97c2c49",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.700055Z",
          "iopub.status.busy": "2022-09-07T22:04:01.699577Z",
          "iopub.status.idle": "2022-09-07T22:04:01.734203Z",
          "shell.execute_reply": "2022-09-07T22:04:01.733315Z"
        },
        "origin_pos": 21,
        "tab": [
          "pytorch"
        ],
        "id": "b97c2c49",
        "outputId": "b3afb777-29db-464c-8d48-2fc3015c555b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine sim=0.839: babies\n",
            "cosine sim=0.800: boy\n",
            "cosine sim=0.792: girl\n"
          ]
        }
      ],
      "source": [
        "get_similar_tokens('baby', 3, glove_6b50d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "84b86c3c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.737750Z",
          "iopub.status.busy": "2022-09-07T22:04:01.737191Z",
          "iopub.status.idle": "2022-09-07T22:04:01.770498Z",
          "shell.execute_reply": "2022-09-07T22:04:01.769243Z"
        },
        "origin_pos": 22,
        "tab": [
          "pytorch"
        ],
        "id": "84b86c3c",
        "outputId": "5ed521b6-eb75-434d-e2df-cd115672b31e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine sim=0.921: lovely\n",
            "cosine sim=0.893: gorgeous\n",
            "cosine sim=0.830: wonderful\n"
          ]
        }
      ],
      "source": [
        "get_similar_tokens('beautiful', 3, glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85eb7312",
      "metadata": {
        "origin_pos": 23,
        "id": "85eb7312"
      },
      "source": [
        "### Word Analogy\n",
        "\n",
        "Besides finding similar words,\n",
        "we can also apply word vectors\n",
        "to word analogy tasks.\n",
        "For example,\n",
        "“man”:“woman”::“son”:“daughter”\n",
        "is the form of a word analogy:\n",
        "“man” is to “woman” as “son” is to “daughter”.\n",
        "Specifically,\n",
        "the word analogy completion task\n",
        "can be defined as:\n",
        "for a word analogy \n",
        "$a : b :: c : d$, given the first three words $a$, $b$ and $c$, find $d$. \n",
        "Denote the vector of word $w$ by $\\text{vec}(w)$. \n",
        "To complete the analogy,\n",
        "we will find the word \n",
        "whose vector is most similar\n",
        "to the result of $\\text{vec}(c)+\\text{vec}(b)-\\text{vec}(a)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4a9904ae",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.774927Z",
          "iopub.status.busy": "2022-09-07T22:04:01.774174Z",
          "iopub.status.idle": "2022-09-07T22:04:01.781351Z",
          "shell.execute_reply": "2022-09-07T22:04:01.780184Z"
        },
        "origin_pos": 24,
        "tab": [
          "pytorch"
        ],
        "id": "4a9904ae"
      },
      "outputs": [],
      "source": [
        "def get_analogy(token_a, token_b, token_c, embed):\n",
        "    vecs = embed[[token_a, token_b, token_c]]\n",
        "    x = vecs[1] - vecs[0] + vecs[2]\n",
        "    topk, cos = knn(embed.idx_to_vec, x, 1)\n",
        "    return embed.idx_to_token[int(topk[0])]  # Remove unknown words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f58573c",
      "metadata": {
        "origin_pos": 25,
        "id": "0f58573c"
      },
      "source": [
        "Let's verify the \"male-female\" analogy using the loaded word vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7d0d5561",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.785948Z",
          "iopub.status.busy": "2022-09-07T22:04:01.785033Z",
          "iopub.status.idle": "2022-09-07T22:04:01.812951Z",
          "shell.execute_reply": "2022-09-07T22:04:01.812108Z"
        },
        "origin_pos": 26,
        "tab": [
          "pytorch"
        ],
        "id": "7d0d5561",
        "outputId": "ddada6c4-eac0-4fbe-920e-bcc1e407aaac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'daughter'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "get_analogy('man', 'woman', 'son', glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46fbe267",
      "metadata": {
        "origin_pos": 27,
        "id": "46fbe267"
      },
      "source": [
        "Below completes a\n",
        "“capital-country” analogy: \n",
        "“beijing”:“china”::“tokyo”:“japan”.\n",
        "This demonstrates \n",
        "semantics in the pretrained word vectors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "24c8e8c9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.816808Z",
          "iopub.status.busy": "2022-09-07T22:04:01.816100Z",
          "iopub.status.idle": "2022-09-07T22:04:01.854922Z",
          "shell.execute_reply": "2022-09-07T22:04:01.854084Z"
        },
        "origin_pos": 28,
        "tab": [
          "pytorch"
        ],
        "id": "24c8e8c9",
        "outputId": "204899cd-6cbd-4434-db97-bd8f92d50d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'japan'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "get_analogy('beijing', 'china', 'tokyo', glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98d9a6e1",
      "metadata": {
        "origin_pos": 29,
        "id": "98d9a6e1"
      },
      "source": [
        "For the\n",
        "“adjective-superlative adjective” analogy\n",
        "such as \n",
        "“bad”:“worst”::“big”:“biggest”,\n",
        "we can see that the pretrained word vectors\n",
        "may capture the syntactic information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "17ed751f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.858943Z",
          "iopub.status.busy": "2022-09-07T22:04:01.858259Z",
          "iopub.status.idle": "2022-09-07T22:04:01.883114Z",
          "shell.execute_reply": "2022-09-07T22:04:01.882260Z"
        },
        "origin_pos": 30,
        "tab": [
          "pytorch"
        ],
        "id": "17ed751f",
        "outputId": "a877dbe8-6bcf-4072-d9fa-3730fc549c20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'biggest'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "get_analogy('bad', 'worst', 'big', glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e1a1313",
      "metadata": {
        "origin_pos": 31,
        "id": "4e1a1313"
      },
      "source": [
        "To show the captured notion\n",
        "of past tense in the pretrained word vectors,\n",
        "we can test the syntax using the\n",
        "\"present tense-past tense\" analogy: “do”:“did”::“go”:“went”.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5407232f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-09-07T22:04:01.886850Z",
          "iopub.status.busy": "2022-09-07T22:04:01.886392Z",
          "iopub.status.idle": "2022-09-07T22:04:01.912140Z",
          "shell.execute_reply": "2022-09-07T22:04:01.911270Z"
        },
        "origin_pos": 32,
        "tab": [
          "pytorch"
        ],
        "id": "5407232f",
        "outputId": "5f6dd22f-2412-4fd7-8cd0-ed0bfd96896a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'went'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "get_analogy('do', 'did', 'go', glove_6b50d)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efe500dd",
      "metadata": {
        "origin_pos": 33,
        "id": "efe500dd"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* In practice, word vectors that are pretrained on large corpora can be applied to downstream natural language processing tasks.\n",
        "* Pretrained word vectors can be applied to the word similarity and analogy tasks.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Test the fastText results using `TokenEmbedding('wiki.en')`.\n",
        "1. When the vocabulary is extremely large, how can we find similar words or complete a word analogy faster?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5ec7f2b",
      "metadata": {
        "origin_pos": 35,
        "tab": [
          "pytorch"
        ],
        "id": "f5ec7f2b"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1336)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}